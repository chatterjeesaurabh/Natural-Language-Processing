{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Classifier using LSTM\n",
    "#### Saurabh Chatterjee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**: https://www.kaggle.com/c/fake-news/data#\n",
    "\n",
    "**Dataset Description:** \n",
    "\n",
    "**id**: unique id for a news article \\\n",
    "**title**: the title of a news article \\\n",
    "**author**: author of the news article \\\n",
    "**text**: the text of the article; could be incomplete \\\n",
    "**label**: a label that marks the article as potentially unreliable: \n",
    "- 1: unreliable\n",
    "- 0: reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf     # using version 2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fake_news_dataset/train.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Null values:\n",
    "df.isnull() .sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP the Null/NaN Values (since it is text data and adding new text data may distort performance)\n",
    "df = df.dropna()\n",
    "df.reset_index(inplace=True)         ## RESET INDEX After the ROWS DROPS ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Independent Features\n",
    "X = df.drop('label', axis=1)        # Drop the Label Coluumn \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label / Dependent feature\n",
    "y = df['label']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Size\n",
    "voc_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Going to consider only the 'Title' Column Features of Df messages for Fake News Classifier.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = X.copy()\n",
    "''' Going to consider only the 'Title' Column Features of Df messages for Fake News Classifier.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data: Removing Symbols, Removing Stopwords and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        House Dem Aide: We Didn’t Even See Comey’s Let...\n",
       "1        FLYNN: Hillary Clinton, Big Woman on Campus - ...\n",
       "2                        Why the Truth Might Get You Fired\n",
       "3        15 Civilians Killed In Single US Airstrike Hav...\n",
       "4        Iranian woman jailed for fictional unpublished...\n",
       "                               ...                        \n",
       "18280    Rapper T.I.: Trump a ’Poster Child For White S...\n",
       "18281    N.F.L. Playoffs: Schedule, Matchups and Odds -...\n",
       "18282    Macy’s Is Said to Receive Takeover Approach by...\n",
       "18283    NATO, Russia To Hold Parallel Exercises In Bal...\n",
       "18284                            What Keeps the F-35 Alive\n",
       "Name: title, Length: 18285, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()        # for Lemmatization\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i])         # Replace Characters \"OTHER THAN' (^) a-z and A-Z in the sentence   (Cleaning)\n",
    "    review = review.lower()         # lower the Case\n",
    "    review = review.split()         # Get the WORDS as a LIST (Split based on Space)\n",
    "\n",
    "    # Removing Stop-Words and LEMMATIZATION:\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house dem aide even see comey letter jason chaffetz tweeted',\n",
       " 'flynn hillary clinton big woman campus breitbart',\n",
       " 'truth might get fired',\n",
       " 'civilian killed single u airstrike identified',\n",
       " 'iranian woman jailed fictional unpublished story woman stoned death adultery',\n",
       " 'jackie mason hollywood would love trump bombed north korea lack trans bathroom exclusive video breitbart',\n",
       " 'beno hamon win french socialist party presidential nomination new york time',\n",
       " 'back channel plan ukraine russia courtesy trump associate new york time',\n",
       " 'obama organizing action partner soros linked indivisible disrupt trump agenda',\n",
       " 'bbc comedy sketch real housewife isi cause outrage',\n",
       " 'russian researcher discover secret nazi military base treasure hunter arctic photo',\n",
       " 'u official see link trump russia',\n",
       " 'yes paid government troll social medium blog forum website',\n",
       " 'major league soccer argentine find home success new york time',\n",
       " 'well fargo chief abruptly step new york time',\n",
       " 'anonymous donor pay million release everyone arrested dakota access pipeline',\n",
       " 'fbi close hillary',\n",
       " 'chuck todd buzzfeed donald trump political favor breitbart',\n",
       " 'monica lewinsky clinton sex scandal set american crime story',\n",
       " 'rob reiner trump mentally unstable breitbart',\n",
       " 'abortion pill order rise latin american nation zika alert new york time',\n",
       " 'nuke un historic treaty ban nuclear weapon',\n",
       " 'exclusive islamic state supporter vow shake west following manchester terrorist massacre breitbart',\n",
       " 'humiliated hillary try hide camera caught min rally',\n",
       " 'andrea tantaros fox news claim retaliation sex harassment complaint new york time',\n",
       " 'hillary clinton became hawk new york time',\n",
       " 'chuck todd buzzfeed eic published fake news breitbart',\n",
       " 'boris johnson brexit leader fumble new york time',\n",
       " 'texas oil field rebound price lull job left behind new york time',\n",
       " 'bayer deal monsanto follows agribusiness trend raising worry farmer new york time']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:30]      # title data after cleaning: first 30 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Embeddings and Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot                # One-Hot Encoder (Keras)\n",
    "from keras_preprocessing.sequence import pad_sequences      # Pre and Post Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2339, 187, 1327, 928, 2942, 2250, 3099, 96, 465, 1143],\n",
       " [2270, 4421, 1345, 1369, 865, 212, 331],\n",
       " [3550, 805, 4446, 1386],\n",
       " [610, 1808, 3292, 3873, 858, 4672],\n",
       " [1299, 865, 1154, 3384, 3908, 4748, 865, 1040, 2529, 3549],\n",
       " [4193,\n",
       "  2994,\n",
       "  2725,\n",
       "  1423,\n",
       "  3060,\n",
       "  2647,\n",
       "  3897,\n",
       "  587,\n",
       "  2322,\n",
       "  4192,\n",
       "  2438,\n",
       "  2263,\n",
       "  2649,\n",
       "  2578,\n",
       "  331],\n",
       " [1885, 3038, 3410, 894, 1509, 401, 1730, 4753, 3274, 1696, 1503],\n",
       " [178, 3138, 1036, 927, 1354, 1163, 2647, 1560, 3274, 1696, 1503],\n",
       " [2692, 1566, 2829, 2884, 1434, 3989, 3577, 4569, 2647, 2082],\n",
       " [1672, 3533, 412, 1835, 4193, 2680, 4750, 3815],\n",
       " [4456, 4314, 2806, 3451, 166, 620, 3936, 1521, 2688, 1945, 3786],\n",
       " [3873, 3671, 2942, 1736, 2647, 1354],\n",
       " [3675, 2736, 4788, 1909, 3132, 1818, 4108, 3730, 1894],\n",
       " [438, 2113, 4077, 799, 2291, 3970, 2389, 3274, 1696, 1503],\n",
       " [1921, 4747, 80, 4046, 230, 3274, 1696, 1503],\n",
       " [706, 809, 1637, 832, 21, 264, 1058, 2163, 4702, 4635],\n",
       " [1508, 3844, 4421],\n",
       " [3306, 2451, 3529, 3046, 2647, 125, 3413, 331],\n",
       " [2401, 4099, 1345, 4327, 3049, 3349, 4607, 63, 4748],\n",
       " [4158, 1762, 2647, 2165, 4638, 331]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Representation:\n",
    "onehot_repr = [one_hot(words, voc_size) for words in corpus]    # returns one-hot vector 1-INDICES AS A LIST of size voc_size (500) for each Sentence\n",
    "onehot_repr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 1907 4486 2798]\n",
      " [   0    0    0 ...  329 4518 2028]\n",
      " [   0    0    0 ... 4745 3732  172]\n",
      " ...\n",
      " [   0    0    0 ... 1159 3561 1145]\n",
      " [   0    0    0 ... 3408  911  845]\n",
      " [   0    0    0 ... 3517 3837 4277]]\n"
     ]
    }
   ],
   "source": [
    "# PADDING: To make Length of all Sentences Equal\n",
    "\n",
    "sent_length = 20     # set Max Sentence Length\n",
    "embedded_docs = pad_sequences(onehot_repr, padding='pre', maxlen=sent_length)       # Pre-Padding\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To represent EACH WORD: Feature Vector Size (like Word2Vec)\n",
    "embedding_dim = 40        # sets the Embedding Layer Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding                          # Embedding Layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 20, 40)            200000    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               56400     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 257,421\n",
      "Trainable params: 257,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embedding_dim, input_length=sent_length))      ## Creates EMBEDDING Weight Layer of DIMENSION: ** (voc_size, embedding_dim) ** (5000, 20)\n",
    "model.add(LSTM(100))                            # LSTM\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))        # Classification layer\n",
    "\n",
    "\"\"\" The Embedding Layer will take as input an integer matrix of size (batch, input_length=20), and the largest integer (i.e. word index) in the input \n",
    "should be no larger than vocabulary size. Now model.output_shape is (None, input_length=20, embedding_dim=40), where (input_length = sent_length = 20) and `None` is the batch dimension.\"\"\"\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18285, 20), (18285,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Training and Test\n",
    "X_final = np.array(embedded_docs)       # shape: (18285, 20)\n",
    "y_final = np.array(y)\n",
    "\n",
    "X_final.shape, y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "192/192 [==============================] - 5s 10ms/step - loss: 0.3571 - accuracy: 0.8464 - val_loss: 0.2294 - val_accuracy: 0.9079\n",
      "Epoch 2/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.1342 - accuracy: 0.9458 - val_loss: 0.2738 - val_accuracy: 0.9148\n",
      "Epoch 3/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0740 - accuracy: 0.9741 - val_loss: 0.2449 - val_accuracy: 0.9188\n",
      "Epoch 4/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0371 - accuracy: 0.9889 - val_loss: 0.3314 - val_accuracy: 0.9127\n",
      "Epoch 5/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0210 - accuracy: 0.9940 - val_loss: 0.3501 - val_accuracy: 0.9117\n",
      "Epoch 6/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 0.3577 - val_accuracy: 0.9042\n",
      "Epoch 7/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.4981 - val_accuracy: 0.9102\n",
      "Epoch 8/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.6238 - val_accuracy: 0.9140\n",
      "Epoch 9/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 9.9063e-04 - accuracy: 0.9999 - val_loss: 0.6239 - val_accuracy: 0.9105\n",
      "Epoch 10/10\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 3.6247e-04 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 0.9114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1375b5abee0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try adding DROPOUT\n",
    "from tensorflow.keras.layers import Dropout\n",
    "## Creating model\n",
    "embedding_vector_features=40\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Test Set Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Set y=1 where y_pred output (probability) > 0.5 else 0\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3115,  304],\n",
       "       [ 231, 2385]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)        # Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9113504556752279\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      3419\n",
      "           1       0.89      0.91      0.90      2616\n",
      "\n",
      "    accuracy                           0.91      6035\n",
      "   macro avg       0.91      0.91      0.91      6035\n",
      "weighted avg       0.91      0.91      0.91      6035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
