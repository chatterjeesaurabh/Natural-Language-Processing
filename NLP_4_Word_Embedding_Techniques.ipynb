{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP: Word Embedding Techniques using Embedding Layer in Keras\n",
    "##### Saurabh Chatterjee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')      # check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot       # One-Hot Encoder (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentences: to be converted into vectors\n",
    "sent=[  'the glass of milk',\n",
    "    'the glass of juice',\n",
    "    'the cup of tea',\n",
    "    'I am a good boy',\n",
    "    'I am a good developer',\n",
    "    'understand the meaning of words',\n",
    "    'your videos are good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Size\n",
    "voc_size = 500          # One-Hot Vector Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[491, 292, 35, 136], [491, 292, 35, 474], [491, 193, 35, 429], [235, 236, 53, 416, 445], [235, 236, 53, 416, 425], [209, 491, 359, 35, 333], [43, 380, 113, 416]]\n"
     ]
    }
   ],
   "source": [
    "onehot_repr = [one_hot(words, voc_size) for words in sent]      # returns one-hot vector 1-INDICES AS A LIST of size voc_size (500) for each Sentence\n",
    "print(onehot_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras_preprocessing.sequence import pad_sequences      # Pre and Post Padding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0 491 292  35 136]\n",
      " [  0   0   0   0 491 292  35 474]\n",
      " [  0   0   0   0 491 193  35 429]\n",
      " [  0   0   0 235 236  53 416 445]\n",
      " [  0   0   0 235 236  53 416 425]\n",
      " [  0   0   0 209 491 359  35 333]\n",
      " [  0   0   0   0  43 380 113 416]]\n"
     ]
    }
   ],
   "source": [
    "# PADDING: To make Length of all Sentences Equal\n",
    "\n",
    "sent_length = 8     # set max sentence length\n",
    "embedded_docs = pad_sequences(onehot_repr, padding='pre', maxlen=sent_length)       # Pre-Padding\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To represent EACH WORD: Feature Vector Size (like Word2Vec)\n",
    "embedding_dim = 10        # sets the Embedding Layer Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The model will take as input an integer matrix of size (batch, input_length), and the largest integer (i.e. word index) in the input \\nshould be no larger than vocabulary size. Now model.output_shape is (None, input_length, embedding_dim), where (input_length = sent_length) and `None` is the batch dimension.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()        # create sequential group object\n",
    "embedding_layer = Embedding(voc_size, embedding_dim, input_length=sent_length)      ## Creates EMBEDDING Weight Layer of DIMENSION: ** (voc_size, embedding_dim) ** (500, 10)\n",
    "model.add(embedding_layer)      # Add EMBEDDING LAYER: \n",
    "model.compile(loss='mse', optimizer='adam')     # set hyperparameters\n",
    "\n",
    "\"\"\" The model will take as input an integer matrix of size (batch, input_length), and the largest integer (i.e. word index) in the input \n",
    "should be no larger than vocabulary size. Now model.output_shape is (None, input_length, embedding_dim), where (input_length = sent_length) and `None` is the batch dimension.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 8, 10)             5000      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,000\n",
      "Trainable params: 5,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 10)\n"
     ]
    }
   ],
   "source": [
    "print( embedding_layer.get_weights()[0].shape)      ## Embedding Weight Layer Dimension: ** (voc_size, embedding_dim) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0, 491, 292,  35, 136])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E.g sentence: 'the glass of milk',\n",
    "embedded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None,).\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03296963, -0.02528973,  0.01243914,  0.0450856 ,  0.04724941,\n",
       "         0.00741047, -0.00646191, -0.04928155,  0.04532487, -0.04230431],\n",
       "       [ 0.03296963, -0.02528973,  0.01243914,  0.0450856 ,  0.04724941,\n",
       "         0.00741047, -0.00646191, -0.04928155,  0.04532487, -0.04230431],\n",
       "       [ 0.03296963, -0.02528973,  0.01243914,  0.0450856 ,  0.04724941,\n",
       "         0.00741047, -0.00646191, -0.04928155,  0.04532487, -0.04230431],\n",
       "       [ 0.03296963, -0.02528973,  0.01243914,  0.0450856 ,  0.04724941,\n",
       "         0.00741047, -0.00646191, -0.04928155,  0.04532487, -0.04230431],\n",
       "       [-0.02349428, -0.02669402, -0.04313287,  0.01782418, -0.02239888,\n",
       "        -0.03605409,  0.04890485,  0.00409625, -0.00211704,  0.03888908],\n",
       "       [-0.03141445,  0.03040603,  0.01714956,  0.00808753,  0.03363431,\n",
       "         0.01334251,  0.01038833,  0.0342663 ,  0.04216622, -0.01509238],\n",
       "       [-0.02581209,  0.01779388, -0.03909098, -0.04414156, -0.00132989,\n",
       "         0.04970746, -0.03879775, -0.01216512,  0.03672105,  0.03227606],\n",
       "       [-0.04530144,  0.00760627,  0.01623261,  0.04204093,  0.02479571,\n",
       "        -0.0174528 ,  0.00607578,  0.04423357, -0.00510848, -0.03331558]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(embedded_docs[0])     # gives Vector Embeddings of Size 10 for Each Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[[[ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [-2.34942790e-02 -2.66940240e-02 -4.31328677e-02  1.78241841e-02\n",
      "   -2.23988779e-02 -3.60540859e-02  4.89048474e-02  4.09624726e-03\n",
      "   -2.11703777e-03  3.88890766e-02]\n",
      "  [-3.14144492e-02  3.04060318e-02  1.71495564e-02  8.08752701e-03\n",
      "    3.36343087e-02  1.33425109e-02  1.03883259e-02  3.42662968e-02\n",
      "    4.21662219e-02 -1.50923841e-02]\n",
      "  [-2.58120894e-02  1.77938826e-02 -3.90909798e-02 -4.41415571e-02\n",
      "   -1.32988766e-03  4.97074611e-02 -3.87977473e-02 -1.21651180e-02\n",
      "    3.67210545e-02  3.22760604e-02]\n",
      "  [-4.53014374e-02  7.60626793e-03  1.62326135e-02  4.20409329e-02\n",
      "    2.47957148e-02 -1.74527988e-02  6.07577711e-03  4.42335717e-02\n",
      "   -5.10847569e-03 -3.33155766e-02]]\n",
      "\n",
      " [[ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [-2.34942790e-02 -2.66940240e-02 -4.31328677e-02  1.78241841e-02\n",
      "   -2.23988779e-02 -3.60540859e-02  4.89048474e-02  4.09624726e-03\n",
      "   -2.11703777e-03  3.88890766e-02]\n",
      "  [-3.14144492e-02  3.04060318e-02  1.71495564e-02  8.08752701e-03\n",
      "    3.36343087e-02  1.33425109e-02  1.03883259e-02  3.42662968e-02\n",
      "    4.21662219e-02 -1.50923841e-02]\n",
      "  [-2.58120894e-02  1.77938826e-02 -3.90909798e-02 -4.41415571e-02\n",
      "   -1.32988766e-03  4.97074611e-02 -3.87977473e-02 -1.21651180e-02\n",
      "    3.67210545e-02  3.22760604e-02]\n",
      "  [ 1.81762092e-02  3.15848328e-02 -3.50252762e-02  4.82019894e-02\n",
      "    9.08337533e-04 -3.13179120e-02 -8.14453512e-03 -4.75598350e-02\n",
      "    4.64587919e-02 -2.58227438e-03]]\n",
      "\n",
      " [[ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [-2.34942790e-02 -2.66940240e-02 -4.31328677e-02  1.78241841e-02\n",
      "   -2.23988779e-02 -3.60540859e-02  4.89048474e-02  4.09624726e-03\n",
      "   -2.11703777e-03  3.88890766e-02]\n",
      "  [ 1.29806064e-02 -1.43982396e-02 -3.20868380e-02 -2.61630658e-02\n",
      "   -3.18674594e-02 -1.33596361e-04  9.99193266e-03  1.53523348e-02\n",
      "   -2.63263229e-02  4.12682444e-03]\n",
      "  [-2.58120894e-02  1.77938826e-02 -3.90909798e-02 -4.41415571e-02\n",
      "   -1.32988766e-03  4.97074611e-02 -3.87977473e-02 -1.21651180e-02\n",
      "    3.67210545e-02  3.22760604e-02]\n",
      "  [-1.66263096e-02  3.85698415e-02 -4.13004309e-03  1.78484581e-02\n",
      "   -3.92274037e-02  3.29126008e-02 -4.03370373e-02 -1.93009265e-02\n",
      "   -4.43378948e-02 -4.53431606e-02]]\n",
      "\n",
      " [[ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [-8.39421898e-03 -1.20335594e-02  1.82835944e-02  3.87429483e-02\n",
      "    3.55708040e-02 -8.04262236e-03 -1.83602683e-02 -1.08939894e-02\n",
      "   -7.77097791e-03  4.45673801e-02]\n",
      "  [-3.32584381e-02 -2.29512528e-03 -3.73016112e-02  4.48591635e-03\n",
      "   -2.74916291e-02 -1.93112027e-02  1.30879879e-03  1.75959803e-02\n",
      "   -2.43692640e-02  4.25396599e-02]\n",
      "  [-3.82773578e-05 -3.16019431e-02 -3.11647542e-02 -1.50670521e-02\n",
      "   -3.91150117e-02  2.14515217e-02  6.84441254e-03  2.20452212e-02\n",
      "    3.09806801e-02  3.14391516e-02]\n",
      "  [ 3.63483280e-03 -1.62832849e-02  1.12007372e-02  3.37816738e-02\n",
      "    3.43079083e-02  1.29217841e-02  1.39652602e-02 -1.09425075e-02\n",
      "    2.62205638e-02 -7.11703300e-03]\n",
      "  [-2.60541923e-02  1.35883950e-02  2.30040401e-03  2.49583013e-02\n",
      "   -4.08753529e-02 -1.83953531e-02  3.34174149e-02 -4.43376303e-02\n",
      "   -2.98011787e-02  4.95620631e-02]]\n",
      "\n",
      " [[ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [-8.39421898e-03 -1.20335594e-02  1.82835944e-02  3.87429483e-02\n",
      "    3.55708040e-02 -8.04262236e-03 -1.83602683e-02 -1.08939894e-02\n",
      "   -7.77097791e-03  4.45673801e-02]\n",
      "  [-3.32584381e-02 -2.29512528e-03 -3.73016112e-02  4.48591635e-03\n",
      "   -2.74916291e-02 -1.93112027e-02  1.30879879e-03  1.75959803e-02\n",
      "   -2.43692640e-02  4.25396599e-02]\n",
      "  [-3.82773578e-05 -3.16019431e-02 -3.11647542e-02 -1.50670521e-02\n",
      "   -3.91150117e-02  2.14515217e-02  6.84441254e-03  2.20452212e-02\n",
      "    3.09806801e-02  3.14391516e-02]\n",
      "  [ 3.63483280e-03 -1.62832849e-02  1.12007372e-02  3.37816738e-02\n",
      "    3.43079083e-02  1.29217841e-02  1.39652602e-02 -1.09425075e-02\n",
      "    2.62205638e-02 -7.11703300e-03]\n",
      "  [-4.82599139e-02  4.43585850e-02 -1.98267940e-02 -3.56690772e-02\n",
      "   -4.27598022e-02  3.58307473e-02 -2.08084118e-02  3.79744805e-02\n",
      "    2.88119800e-02 -3.27943563e-02]]\n",
      "\n",
      " [[ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 2.64124535e-02 -2.78064851e-02  4.58408631e-02 -2.03993917e-02\n",
      "   -6.64909929e-03 -1.89318545e-02  3.90153416e-02  4.34888862e-02\n",
      "   -1.95926912e-02 -1.56266317e-02]\n",
      "  [-2.34942790e-02 -2.66940240e-02 -4.31328677e-02  1.78241841e-02\n",
      "   -2.23988779e-02 -3.60540859e-02  4.89048474e-02  4.09624726e-03\n",
      "   -2.11703777e-03  3.88890766e-02]\n",
      "  [ 1.51717179e-02  4.30899374e-02 -1.76990740e-02  3.21884789e-02\n",
      "   -4.75347042e-02 -4.32139412e-02 -3.07167526e-02  3.58893760e-02\n",
      "   -1.19941719e-02 -4.71170321e-02]\n",
      "  [-2.58120894e-02  1.77938826e-02 -3.90909798e-02 -4.41415571e-02\n",
      "   -1.32988766e-03  4.97074611e-02 -3.87977473e-02 -1.21651180e-02\n",
      "    3.67210545e-02  3.22760604e-02]\n",
      "  [-2.93015316e-03  1.89805739e-02 -1.23748779e-02 -3.55330482e-02\n",
      "    2.88250558e-02  3.29987518e-02 -3.60040069e-02  3.52453701e-02\n",
      "    4.41830233e-03 -2.79313810e-02]]\n",
      "\n",
      " [[ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 3.29696275e-02 -2.52897274e-02  1.24391429e-02  4.50855978e-02\n",
      "    4.72494103e-02  7.41046667e-03 -6.46190718e-03 -4.92815487e-02\n",
      "    4.53248657e-02 -4.23043147e-02]\n",
      "  [ 2.40623243e-02 -1.06649175e-02  1.23909824e-02  3.90495323e-02\n",
      "   -4.59418185e-02 -3.77902389e-02  1.51559450e-02  2.41512768e-02\n",
      "   -2.40280982e-02 -2.83939485e-02]\n",
      "  [ 5.51619381e-03 -3.32427397e-02  3.94229554e-02  2.08503865e-02\n",
      "    3.09617780e-02 -2.15351116e-02 -8.20241123e-03  9.96500254e-03\n",
      "    4.67304140e-03 -9.98427719e-03]\n",
      "  [ 4.47424538e-02 -2.58787721e-03 -4.55357917e-02 -4.76362966e-02\n",
      "    1.80144645e-02 -4.57363129e-02 -3.78418677e-02  2.41403319e-02\n",
      "    2.97509916e-02  7.56484270e-03]\n",
      "  [ 3.63483280e-03 -1.62832849e-02  1.12007372e-02  3.37816738e-02\n",
      "    3.43079083e-02  1.29217841e-02  1.39652602e-02 -1.09425075e-02\n",
      "    2.62205638e-02 -7.11703300e-03]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Embedding Layer is Trained along with rest of the neural network layers with Classification Loss which makes it automatically learn \n",
    "semantic relations between words and assign Vectors accordingly, means Similar Words will be assigned Nearby Vectors.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
